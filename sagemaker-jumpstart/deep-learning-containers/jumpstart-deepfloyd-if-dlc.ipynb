{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Deep Floyd IF using Stability AI DLC on AWS SageMaker\n",
    "### Example: Deep Floyd IF v1.0 on PyTorch 2.0 with Diffusers\n",
    "\n",
    "This example will deploy an endpoint running Stable Diffusion 2.1 with an easy-to-use HTTP JSON interface that mirrors the [Stability AI REST API](https://api.stability.ai/docs).\n",
    "\n",
    "The [Stability SDK](https://github.com/Stability-AI/stability-sdk) provides this contract and is pre-installed in the Stability AI DLC.\n",
    "\n",
    "**The Deep Floyd weights are only available for non-commercial, research use. Weights must be downloading from the HuggingFace Hub by supplying a token, or otherwise provided. Use of the weights with this container must follow the agreed license.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker huggingface_hub \"stability-sdk[sagemaker] @ git+https://github.com/Stability-AI/stability-sdk.git@sagemaker\" --upgrade --quiet\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role\n",
    "from stability_sdk_sagemaker.predictor import StabilityPredictor\n",
    "from stability_sdk_sagemaker.models import get_model_package_arn\n",
    "from stability_sdk.api import GenerationRequest, GenerationResponse, TextPrompt\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "import boto3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the model weights.\n",
    "\n",
    "You will need to add your ðŸ¤— Hub token to download the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = 'YOUR_TOKEN_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "local_dir = './model/if'\n",
    "allow_patterns=[\"*.json\", \"*.fp16*safetensors\", \"watermarker/diffusion_pytorch_model.safetensors\", \"tokenizer/spiece.model\"]\n",
    "snapshot_download(\n",
    "    repo_id=\"DeepFloyd/IF-I-XL-v1.0\",    \n",
    "    allow_patterns=allow_patterns,\n",
    "    local_dir=os.path.join(local_dir, 'IF-I-XL'),\n",
    "    local_dir_use_symlinks=False)\n",
    "snapshot_download(\n",
    "    repo_id=\"DeepFloyd/IF-II-L-v1.0\",\n",
    "    allow_patterns=allow_patterns,\n",
    "    ignore_patterns=[\"text_encoder/*\"],\n",
    "    local_dir=os.path.join(local_dir, 'IF-II-L'),\n",
    "    local_dir_use_symlinks=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create custom inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model/code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Script: Stability API contract, Text2Image, Image2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/code/deepfloyd_if_inference.py\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.utils import pt_to_pil\n",
    "from io import BytesIO\n",
    "import base64, torch, os, time, json, uuid\n",
    "from stability_sdk.api import CreateRequest, CreateResponse\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    if_model_dir = os.path.join(model_dir, 'if')    \n",
    "    stage_1 = DiffusionPipeline.from_pretrained(os.path.join(if_model_dir, \"IF-I-XL\"), variant=\"fp16\", torch_dtype=torch.float16, local_files_only=True)\n",
    "    \n",
    "    # Enabling CPU offload allows stage 1 and 2 to run on a single NVIDIA A10 g5.2xl instance. For better performance, \n",
    "    # deploy each stage as a distinct model on separate g5.xlarge endpoints and chain them together.\n",
    "    stage_1.enable_model_cpu_offload()        \n",
    "                                                  \n",
    "    stage_2 = DiffusionPipeline.from_pretrained(\n",
    "        os.path.join(if_model_dir, \"IF-II-L\"), text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, local_files_only=True\n",
    "    )\n",
    "    stage_2.enable_model_cpu_offload()\n",
    "    \n",
    "    # Stage 3 causes generation time to exceed 1m which currently is not possible on realtime endpoints, so it's disabled for this example.\n",
    "    # It is possible to use stage 3 on g5.4xlarge with async inference.\n",
    "    \n",
    "    # safety_modules = {\"feature_extractor\": stage_1.feature_extractor, \"safety_checker\": stage_1.safety_checker, \"watermarker\": stage_1.watermarker}                                              \n",
    "    # stage_3 = DiffusionPipeline.from_pretrained(os.path.join(if_model_dir, \"sd-x4-upscaler\", **safety_modules, torch_dtype=torch.float16, local_files_only=True)\n",
    "    # stage_3.enable_model_cpu_offload()    \n",
    "    stage_3 = None\n",
    "                                              \n",
    "    return {'stage_1': stage_1, 'stage_2': stage_2, 'stage_3': stage_3}\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == \"application/json\":\n",
    "        model_input = json.loads(request_body)\n",
    "        request = CreateRequest(model_input)\n",
    "        return request\n",
    "    if request_content_type == \"application/x-protobuf\":\n",
    "        request = generation.Request()\n",
    "        request.ParseFromString(request_body)\n",
    "        return request\n",
    "    raise Exception(\"Content-type must be application/json\")\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    start_time = time.time()\n",
    "    prompt = input_object.prompt[0].text\n",
    "    seed = 0\n",
    "    image_params = input_object.image\n",
    "    if (image_params):\n",
    "        if image_params.seed and image_params.seed[0]:\n",
    "            seed = image_params.seed[0]                                \n",
    "    \n",
    "    prompt_embeds, negative_embeds = model['stage_1'].encode_prompt(prompt)\n",
    "    generator = torch.manual_seed(seed)\n",
    "    image = model['stage_1'](prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\").images\n",
    "    image = model['stage_2'](image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\").images\n",
    "    \n",
    "    # Skip the upscaler if not loaded\n",
    "    if model['stage_3']:\n",
    "        image = model['stage_3'](prompt=prompt, image=image, generator=generator, noise_level=100).images\n",
    "    else:\n",
    "        image = pt_to_pil(image)\n",
    "    batch = generation.AnswerBatch()\n",
    "    batch.batch_id = input_object.request_id\n",
    "    image_buf = BytesIO()\n",
    "    image[0].save(image_buf, format=\"PNG\")\n",
    "    artifact = generation.Artifact(\n",
    "        type=generation.ARTIFACT_IMAGE,\n",
    "        mime=\"image/png\",\n",
    "        binary=image_buf.getvalue(),\n",
    "        finish_reason=generation.NULL)\n",
    "    answer = generation.Answer(\n",
    "        answer_id = str(uuid.uuid4()),\n",
    "        request_id = input_object.request_id,\n",
    "        created=int(time.time() * 1000),\n",
    "        received=int(start_time * 1000))\n",
    "    answer.artifacts.append(artifact)\n",
    "    batch.answers.append(answer)\n",
    "    return batch\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    if accept == \"application/x-protobuf\":\n",
    "        return prediction.SerializeToString(), accept    \n",
    "    response = CreateResponse(prediction.answers[0])\n",
    "    if response.result == \"error\" or accept.startswith('application/json'):        \n",
    "        return response.json(exclude_unset=True), accept\n",
    "    else: # Default to image/png\n",
    "        return prediction.answers[0].artifacts[0].binary, accept\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Package and upload model archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "model_package_name = name_from_base(f\"deepfloyd-if\")\n",
    "model_uri = f's3://{sagemaker_session_bucket}/{model_package_name}/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Packaging and uploading model to {model_uri}, this will take a while...')\n",
    "!tar -cf - -C model if code | gzip --fast | aws s3 cp - {model_uri}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and deploy a model and perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Stability AI DLC inference image\n",
    "inference_image_uri = '740929234339.dkr.ecr.us-east-1.amazonaws.com/stabilityai-pytorch-inference:2.0.0-diffusers0.17.0-gpu-py310-cu118-ubuntu20.04-2023-06-12-14-30-49'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "endpoint_name = name_from_base(f\"deepfloyd-if\")\n",
    "# create model class\n",
    "model = Model(\n",
    "   model_data=model_uri,      # path to your model and script\n",
    "   image_uri=inference_image_uri, # path to your private ecr image \n",
    "   env={\n",
    "       \"SAGEMAKER_PROGRAM\": \"deepfloyd_if_inference.py\",  # override inference with packaged code\n",
    "       \"TS_DEFAULT_RESPONSE_TIMEOUT\": \"1000\",             # increase timeouts \n",
    "   }, \n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   predictor_cls=StabilityPredictor  # StabilityPredictor provides serialization\n",
    ")\n",
    "\n",
    "# Deploy the endpoint \n",
    "deployed_model = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also attach the predictor to an existing endpoint by name\n",
    "#deployed_model = StabilityPredictor(endpoint_name=deployed_model.endpoint_name)\n",
    "deployed_model.endpoint_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Text to image\n",
    "\n",
    "The first inference request may time out due to deferred loading of the weights. Subsequent requests should succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"A photograph of fresh pizza with basil and tomatoes, from a traditional oven\")],                                             \n",
    "                                             seed = 2\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_show(model_response: GenerationResponse) -> None:\n",
    "    \"\"\"\n",
    "    Decodes and displays an image from SDXL output\n",
    "\n",
    "    Args:\n",
    "        model_response (GenerationResponse): The response object from the deployed SDXL model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = model_response.artifacts[0].base64\n",
    "    image_data = base64.b64decode(image.encode())\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    display(image)\n",
    "\n",
    "decode_and_show(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Image to image\n",
    "\n",
    "Image to image is not supported with the supplied inference script, so these examples will not work as expected currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path: str, resize: bool = True) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Encode an image as a base64 string, optionally resizing it to 512x512.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "        resize (bool, optional): Whether to resize the image. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The encoded image as a string, or None if encoding failed.\n",
    "    \"\"\"\n",
    "    assert os.path.exists(image_path)\n",
    "\n",
    "    if resize:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((512, 512))\n",
    "        image_base = os.path.splitext(image_path)[0]\n",
    "        image_resized_path = f\"{image_base}_resized.png\"\n",
    "        image.save(image_resized_path)\n",
    "        image_path = image_resized_path\n",
    "    image = Image.open(image_path)\n",
    "    assert image.size == (512, 512)\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        img_byte_array = image_file.read()\n",
    "        # Encode the byte array as a Base64 string\n",
    "        try:\n",
    "            base64_str = base64.b64encode(img_byte_array).decode(\"utf-8\")\n",
    "            return base64_str\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to encode image {image_path} as base64 string.\")\n",
    "            print(e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://platform.stability.ai/Cat_August_2010-4.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the original image:\n",
    "display(Image.open('Cat_August_2010-4.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_path = \"Cat_August_2010-4.jpg\"\n",
    "cat_data = encode_image(cat_path)\n",
    "\n",
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"cat in watercolour\")],\n",
    "                                                  init_image= cat_data,\n",
    "                                                  cfg_scale=9,\n",
    "                                                  image_strength=0.8,\n",
    "                                                  seed=42\n",
    "                                                  ))\n",
    "decode_and_show(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"cat painted by Basquiat\")],\n",
    "                                                  init_image= cat_data,\n",
    "                                            cfg_scale=17,\n",
    "                                            image_strength=0.4,\n",
    "                                             seed=42\n",
    "                                             ))\n",
    "decode_and_show(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker list-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete an endpoint\n",
    "deployed_model.sagemaker_session.delete_endpoint(deployed_model.endpoint_name)\n",
    "\n",
    "# Rerun the aws cli command above to confirm that its gone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
