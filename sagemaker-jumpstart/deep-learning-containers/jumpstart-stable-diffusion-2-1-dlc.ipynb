{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d04b5736-0e0e-4b5d-a83d-2b5265489d76",
   "metadata": {},
   "source": [
    "# Deploying Stable Diffusion using Stability AI DLC on AWS SageMaker\n",
    "\n",
    "## Example: Stable Diffusion 2.1 on PyTorch 1.13.1 with Diffusers and Xformers\n",
    "\n",
    "This example will deploy an endpoint running Stable Diffusion 2.1 with an easy-to-use HTTP JSON interface that mirrors the [Stability AI REST API](https://api.stability.ai/docs).\n",
    "\n",
    "The [Stability SDK](https://github.com/Stability-AI/stability-sdk) provides this contract and is pre-installed in the Stability AI DLC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6c8d5-23e0-49e4-b554-28cbc671a743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.116.0\" \"huggingface_hub==0.10.1\" \"stability-sdk[sagemaker] @ git+https://github.com/Stability-AI/stability-sdk.git@sagemaker\" --upgrade --quiet\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role\n",
    "from stability_sdk_sagemaker.predictor import StabilityPredictor\n",
    "from stability_sdk_sagemaker.models import get_model_package_arn\n",
    "from stability_sdk.api import GenerationRequest, GenerationResponse, TextPrompt\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "import boto3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10edf97d-2751-41a7-9d2f-dd8cba8f2c1f",
   "metadata": {},
   "source": [
    "## 1. Custom Inference Script Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38662e-1459-49c0-a0d1-ca84e48c7336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22537ff5-c48b-40e4-9bb2-f5585407e478",
   "metadata": {},
   "source": [
    "### Inference Script: Stability API contract, Text2Image, Image2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c695fe0-9192-46da-84be-8d33b8d94e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/stable_diffusion_inference.py\n",
    "import base64\n",
    "import torch\n",
    "from io import BytesIO\n",
    "import json\n",
    "from PIL import Image\n",
    "from pydantic import Field, ValidationError\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, StableDiffusionImg2ImgPipeline\n",
    "from stability_sdk.api import GenerationRequest, GenerationResponse, GenerationErrorResponse, BinaryArtifact, TextPrompt\n",
    "\n",
    "\n",
    "# GenerationRequest is the pydantic class used by StabilityPredictor to send requests, so we extend it \n",
    "# to customize validation and defaults. This is optional, but recommended.\n",
    "# For custom implementations, you can extend StabilityPredictor to use your own data model.\n",
    "class DiffusersGenerationRequest(GenerationRequest):\n",
    "    text_prompts: list[TextPrompt] = Field(..., min_items=1, max_items=2)\n",
    "    height: int = Field(512, ge=128, le=1024, multiple_of=64)\n",
    "    width: int = Field(512, ge=128, le=1024, multiple_of=64)\n",
    "    steps: int = Field(30, ge=0, le=150)\n",
    "    samples: int = Field(1, ge=1, le=8)\n",
    "    cfg_scale: float = Field(7.5, ge=0.0, le=35.0)\n",
    "    seed: int = Field(None, ge=0, le=2**32)\n",
    "    init_image: str = Field(None)\n",
    "    image_strength: float = Field(0.8, ge=0.0, le=1.0)    \n",
    "\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    device = \"cuda\"\n",
    "    image2image_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    image2image_pipe.enable_xformers_memory_efficient_attention()\n",
    "    image2image_pipe = image2image_pipe.to(device)\n",
    "\n",
    "    # Load stable diffusion and move it to the GPU\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16)\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "\n",
    "    return { \"text2image\": pipe, \"image2image\": image2image_pipe }\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == \"application/json\":\n",
    "        model_input = json.loads(request_body)                \n",
    "        return model_input\n",
    "    else:\n",
    "        raise Exception(\"Content-type must be application/json\")\n",
    "\n",
    "def output_fn(prediction, accept):    \n",
    "    return prediction.json(exclude_unset=True), accept\n",
    "    \n",
    "def predict_fn(data:DiffusersGenerationRequest , pipe):\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    # Validate the input using the pydantic model\n",
    "    # This is done in predict_fn so we can return a custom error response\n",
    "    try:\n",
    "        request = DiffusersGenerationRequest.parse_obj(data)\n",
    "    except ValidationError as e:\n",
    "        error = e.errors()[0]\n",
    "        error_msg = f'{error[\"loc\"][0]}: {error[\"msg\"]}'\n",
    "        \n",
    "        return GenerationResponse(result=\"error\", error=GenerationErrorResponse(id=\"0\", name=error[\"type\"], message=error_msg))\n",
    "\n",
    "    # weights could be supported using prompt_embeds, for now only 1 positive and 1 negative will be used\n",
    "    prompts = []\n",
    "    negative_prompts = []    \n",
    "    for text_prompt in request.text_prompts:\n",
    "        if text_prompt.weight < 0:                \n",
    "            if len(negative_prompts) == 0:\n",
    "                negative_prompts.append(text_prompt.text)\n",
    "        else:\n",
    "            if len(prompts) == 0:\n",
    "                prompts.append(text_prompt.text)    \n",
    "    \n",
    "    latents = None\n",
    "    seeds = []\n",
    "    seed = request.seed\n",
    "    mode = 'image2image' if request.init_image else 'text2image'\n",
    "    \n",
    "    try:\n",
    "        generator = torch.Generator(device=device)\n",
    "        if mode == 'text2image':\n",
    "            if seed:            \n",
    "                for _ in range(request.samples):\n",
    "                    generator.manual_seed(seed)\n",
    "                    seeds.append(seed)\n",
    "                    \n",
    "                    # this should be random based on the last seed, not incremental\n",
    "                    seed = seed + 1\n",
    "\n",
    "                    image_latents = torch.randn(\n",
    "                        (1, pipe[mode].unet.in_channels, request.height // 8, request.width // 8),\n",
    "                        generator = generator,\n",
    "                        device = device\n",
    "                    )\n",
    "                    latents = image_latents if latents is None else torch.cat((latents, image_latents))                \n",
    "            else:\n",
    "                for _ in range(request.samples):\n",
    "                    # Get a new random seed, store it and use it as the generator state\n",
    "                    _seed = generator.seed()\n",
    "                    seeds.append(_seed)\n",
    "                    generator = generator.manual_seed(_seed)\n",
    "\n",
    "                    image_latents = torch.randn(\n",
    "                        (1, pipe[mode].unet.in_channels, request.height // 8, request.width // 8),\n",
    "                        generator = generator,\n",
    "                        device = device\n",
    "                    )\n",
    "                    latents = image_latents if latents is None else torch.cat((latents, image_latents))\n",
    "            \n",
    "            # run generation with parameters\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                generated_images = pipe['text2image'](\n",
    "                    prompt=[prompts[0]] * request.samples,\n",
    "                    height=request.height,\n",
    "                    width=request.width,\n",
    "                    num_inference_steps=request.steps,\n",
    "                    guidance_scale=request.cfg_scale,                                                \n",
    "                    negative_prompt=[negative_prompts[0]] * request.samples if len(negative_prompts) > 0 else None,\n",
    "                    latents = latents\n",
    "                )[\"images\"]\n",
    "\n",
    "            # create response\n",
    "            artifacts = []                \n",
    "            ix = 0\n",
    "            response_seed = 0\n",
    "            for image in generated_images:\n",
    "                buffered = BytesIO()\n",
    "                image.save(buffered, format=\"PNG\")\n",
    "                if ix in seeds:\n",
    "                    response_seed = seeds[ix]                \n",
    "                artifacts.append(BinaryArtifact(seed=response_seed, base64=base64.b64encode(buffered.getvalue()).decode(), finishReason=\"SUCCESS\"))                                             \n",
    "        else:\n",
    "            # image2image    \n",
    "            seed = seed or generator.seed()\n",
    "            \n",
    "            # run generation with parameters\n",
    "            init_image = base64.b64decode(request.init_image)\n",
    "            buffer = BytesIO(init_image)\n",
    "            init_image = Image.open(buffer).convert(\"RGB\")\n",
    "            init_image = init_image.resize((request.width, request.height))        \n",
    "            \n",
    "            generated_images = pipe['image2image'](\n",
    "                num_images_per_prompt=request.samples,\n",
    "                prompt=prompts[0],\n",
    "                image=init_image,\n",
    "                num_inference_steps=request.steps,\n",
    "                guidance_scale=request.cfg_scale,\n",
    "                strength=request.image_strength,\n",
    "                negative_prompt=negative_prompts[0] if len(negative_prompts) > 0 else None,        \n",
    "            )[\"images\"]\n",
    "\n",
    "            # create response\n",
    "            artifacts = []\n",
    "            for image in generated_images:\n",
    "                buffered = BytesIO()\n",
    "                image.save(buffered, format=\"PNG\")\n",
    "                artifacts.append(BinaryArtifact(seed=seed, base64=base64.b64encode(buffered.getvalue()).decode(), finishReason=\"SUCCESS\"))\n",
    "\n",
    "        return GenerationResponse(result=\"success\", artifacts=artifacts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return GenerationResponse(result=\"error\", error=GenerationErrorResponse(id=\"0\", message=str(e), name=\"inference_error\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f50da4c-45ac-429c-bf78-97ab6df1b1dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker Session Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3d986-6b7f-4308-a2cf-983541040d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8abf9d7c-6406-46bb-ade2-567d04054d10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieving Image URI and Model URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4e941-dcab-4820-81ef-2748b54338a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = 'model-txt2img-stabilityai-stable-diffusion-v2-1-base', \"*\"\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"stable-diffusion-v2-1-base\")\n",
    "\n",
    "# # Retrieve the model uri. This includes the pre-trained model and parameters as well as the inference scripts.\n",
    "# # This includes all dependencies and scripts for model loading, inference handling etc..\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "# model_uri\n",
    "print(model_uri)\n",
    "# Or use your own stored model\n",
    "#s3_model_uri = 's3://sagemaker-us-west-2-499172972132/stable-diffusion-v2-1/model.tar.gz'\n",
    "#print(s3_model_uri)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6dd6072-9ef6-4a32-b93a-87f7067a1183",
   "metadata": {},
   "source": [
    "## Create and deploy a model and perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd680a-7af0-4060-ba62-25341dd58973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_uri\n",
    "inference_image_uri = '740929234339.dkr.ecr.us-east-1.amazonaws.com/stabilityai-pytorch-inference:1.13.1-diffusers0.14.0-gpu-xformers-py39-cu117-ubuntu20.04-2023-06-12-01-22-38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673db27-8c8e-4815-b6b3-afff123dac63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# create PyTorch Model class\n",
    "pytorch_model = PyTorchModel(\n",
    "   model_data=model_uri,      # path to your model and script\n",
    "   image_uri=inference_image_uri, # path to your private ecr image\n",
    "   entry_point = 'stable_diffusion_inference.py', #custom inference script\n",
    "   source_dir = \"./code/\",\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   predictor_cls=StabilityPredictor  # StabilityPredictor provides serialization \n",
    ")\n",
    "\n",
    "# Deploy the endpoint \n",
    "# This will take a while as it repackages the model then waits for deployment.\n",
    "deployed_model = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",    \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd21303-b63a-4ec1-ac2f-83cf79e22084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also attach the predictor to an existing endpoint by name\n",
    "#deployed_model = StabilityPredictor(endpoint_name=deployed_model.endpoint_name)\n",
    "deployed_model.endpoint_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf9378b7-9bdc-41bf-a7b4-401c6de94b1c",
   "metadata": {},
   "source": [
    "## A. Text to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cdb1a-f6aa-4350-b241-ef1b240e789d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"A photograph of fresh pizza with basil and tomatoes, from a traditional oven\")],                                             \n",
    "                                             seed = 2\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce12b64-ac31-45e1-bdd1-318445c71582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_and_show(model_response: GenerationResponse) -> None:\n",
    "    \"\"\"\n",
    "    Decodes and displays an image from SDXL output\n",
    "\n",
    "    Args:\n",
    "        model_response (GenerationResponse): The response object from the deployed SDXL model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = model_response.artifacts[0].base64\n",
    "    image_data = base64.b64decode(image.encode())\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    display(image)\n",
    "\n",
    "decode_and_show(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a22d0135-1a61-4d4e-a555-10465fe40e35",
   "metadata": {},
   "source": [
    "## B. Image to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5faa61-01ee-4bb4-ace7-22c402cc445d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path: str, resize: bool = True) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Encode an image as a base64 string, optionally resizing it to 512x512.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "        resize (bool, optional): Whether to resize the image. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The encoded image as a string, or None if encoding failed.\n",
    "    \"\"\"\n",
    "    assert os.path.exists(image_path)\n",
    "\n",
    "    if resize:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((512, 512))\n",
    "        image_base = os.path.splitext(image_path)[0]\n",
    "        image_resized_path = f\"{image_base}_resized.png\"\n",
    "        image.save(image_resized_path)\n",
    "        image_path = image_resized_path\n",
    "    image = Image.open(image_path)\n",
    "    assert image.size == (512, 512)\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        img_byte_array = image_file.read()\n",
    "        # Encode the byte array as a Base64 string\n",
    "        try:\n",
    "            base64_str = base64.b64encode(img_byte_array).decode(\"utf-8\")\n",
    "            return base64_str\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to encode image {image_path} as base64 string.\")\n",
    "            print(e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184979a1-9072-4e21-903d-ef7d689fe1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! wget https://platform.stability.ai/Cat_August_2010-4.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30c1ff-3762-4ede-ad11-306070d741a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is the original image:\n",
    "display(Image.open('Cat_August_2010-4.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bba40-805b-42ab-a2c9-2eaf5810a0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_path = \"Cat_August_2010-4.jpg\"\n",
    "cat_data = encode_image(cat_path)\n",
    "\n",
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"cat in watercolour\")],\n",
    "                                                  init_image= cat_data,\n",
    "                                                  cfg_scale=9,\n",
    "                                                  image_strength=0.8,\n",
    "                                                  seed=42\n",
    "                                                  ))\n",
    "decode_and_show(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcff5ec-fbe8-4891-9477-31bf8938fc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"cat painted by Basquiat\")],\n",
    "                                                  init_image= cat_data,\n",
    "                                            cfg_scale=17,\n",
    "                                            image_strength=0.4,\n",
    "                                             seed=42\n",
    "                                             ))\n",
    "decode_and_show(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54f8af-0c13-4ef5-9b19-665a8db1befa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker list-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a79e2e-714a-464c-be54-96c60ade8fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete an endpoint\n",
    "deployed_model.sagemaker_session.delete_endpoint(deployed_model.endpoint_name)\n",
    "\n",
    "# Rerun the aws cli command above to confirm that its gone."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
